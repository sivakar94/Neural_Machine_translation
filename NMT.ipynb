{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_second_assignment_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EZqkO7oftqCV",
        "colab_type": "code",
        "outputId": "68fa71e4-51a4-4281-b4a6-95ef709976cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2064
        }
      },
      "cell_type": "code",
      "source": [
        "Sivakar Sivarajah\n",
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class NmtModel(object):\n",
        "  def __init__(self,source_dict,target_dict,use_attention):\n",
        "    self.num_layers = 2\n",
        "    self.hidden_size = 200\n",
        "    self.embedding_size = 100\n",
        "    self.hidden_dropout_rate=0.2\n",
        "    self.embedding_dropout_rate = 0.2\n",
        "    self.max_target_step = 30\n",
        "    self.vocab_target_size = len(target_dict.vocab)\n",
        "    self.vocab_source_size = len(source_dict.vocab)\n",
        "    self.target_dict = target_dict\n",
        "    self.source_dict = source_dict\n",
        "    self.SOS = target_dict.word2ids['<start>']\n",
        "    self.EOS = target_dict.word2ids['<end>']\n",
        "    self.use_attention = use_attention\n",
        "\n",
        "    print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "  def build(self):\n",
        "    self.source_words = tf.placeholder(tf.int32,[None,None],\"source_words\")\n",
        "    self.target_words = tf.placeholder(tf.int32,[None,None],\"target_words\")\n",
        "    self.source_sent_lens = tf.placeholder(tf.int32,[None],\"source_sent_lens\")\n",
        "    self.target_sent_lens = tf.placeholder(tf.int32,[None],\"target_sent_lens\")\n",
        "    self.is_training = tf.placeholder(tf.bool,[],\"is_training\")\n",
        "\n",
        "    self.predictions,self.loss = self.get_predictions_and_loss(self.source_words,self.target_words,self.source_sent_lens,self.target_sent_lens,self.is_training)\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "    self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "    self.sess = tf.Session()\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "  def get_predictions_and_loss(self, source_words,target_words, source_sent_lens,target_sent_lens,is_training):\n",
        "    self.embeddings_target = tf.get_variable(\"embeddings_target\", [self.vocab_target_size, self.embedding_size], dtype=tf.float32)\n",
        "    self.embeddings_source = tf.get_variable(\"embeddings_source\", [self.vocab_source_size, self.embedding_size], dtype=tf.float32)\n",
        "\n",
        "    batch_size = shape(target_words, 0)\n",
        "    max_target_sent_len = shape(target_words, 1)\n",
        "\n",
        "    embedding_keep_prob = 1 - (tf.to_float(is_training) * self.embedding_dropout_rate)\n",
        "    hidden_keep_prob = 1 - (tf.to_float(is_training) * self.hidden_dropout_rate)\n",
        "\n",
        "    source_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_source,source_words),embedding_keep_prob)\n",
        "    target_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_target,target_words),embedding_keep_prob)\n",
        "\n",
        "\n",
        "    encoder_outputs, encode_final_states = self.encoder(source_embs,source_sent_lens,hidden_keep_prob)\n",
        "\n",
        "    time_major_target_embs = tf.transpose(target_embs,[1,0,2])\n",
        "\n",
        "\n",
        "    def _decoder_scan(pre,inputs):\n",
        "      pre_logits, pre_pred, pre_states = pre\n",
        "      step_embeddings = inputs\n",
        "\n",
        "      pred_embeddings = tf.nn.embedding_lookup(self.embeddings_target,pre_pred)\n",
        "\n",
        "      step_embeddings = tf.cond(is_training,lambda :step_embeddings,lambda :pred_embeddings)\n",
        "      curr_logits, curr_states = self.step_decoder(step_embeddings,encoder_outputs,pre_states,hidden_keep_prob)\n",
        "      curr_pred = tf.argmax(curr_logits,1,output_type=tf.int32)\n",
        "\n",
        "      return curr_logits, curr_pred, curr_states\n",
        "\n",
        "    init_logits = tf.zeros([batch_size,self.vocab_target_size])\n",
        "    init_pred = tf.ones([batch_size],tf.int32) * self.SOS\n",
        "\n",
        "    time_major_logits, time_major_preds, _ = tf.scan(_decoder_scan,time_major_target_embs,initializer=(init_logits, init_pred,encode_final_states))\n",
        "    time_major_logits, time_major_preds = tf.stack(time_major_logits),tf.stack(time_major_preds)\n",
        "\n",
        "    logits = tf.transpose(time_major_logits,[1,0,2])\n",
        "    predictions = tf.transpose(time_major_preds,[1,0])\n",
        "\n",
        "    logits_mask = tf.sequence_mask(target_sent_lens-1,max_target_sent_len)\n",
        "    flatten_logits_mask = tf.reshape(logits_mask,[batch_size*max_target_sent_len])\n",
        "    flatten_logits = tf.boolean_mask(tf.reshape(logits,[batch_size*max_target_sent_len,self.vocab_target_size]),flatten_logits_mask)\n",
        "\n",
        "    gold_labels_mask = tf.concat([tf.zeros([batch_size,1],dtype=tf.bool),tf.sequence_mask(target_sent_lens-1,max_target_sent_len-1)],1)\n",
        "    flatten_gold_labels_mask = tf.reshape(gold_labels_mask,[batch_size*max_target_sent_len])\n",
        "    flatten_gold_labels = tf.boolean_mask(tf.reshape(target_words,[batch_size*max_target_sent_len]),flatten_gold_labels_mask)\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=flatten_gold_labels,logits=flatten_logits))\n",
        "\n",
        "    return predictions, loss\n",
        "\n",
        "\n",
        "\n",
        "  def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "      \"\"\"\n",
        "      Task 1 encoder\n",
        "      \n",
        "      Start\n",
        "      \"\"\"\n",
        "      word_lstm1 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                  state_keep_prob=hidden_keep_prob,\n",
        "                                                  variational_recurrent=True,\n",
        "                                                  dtype=tf.float32)\n",
        "      word_lstm2 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                  state_keep_prob=hidden_keep_prob,\n",
        "                                                  variational_recurrent=True,\n",
        "                                                  dtype=tf.float32)\n",
        "      enc_cell =tf.nn.rnn_cell.MultiRNNCell([word_lstm1,word_lstm2])\n",
        "      \n",
        "      (encoder_outputs, encoder_final_states) = tf.nn.dynamic_rnn(cell=enc_cell,inputs=embeddings,\n",
        "                                                                  sequence_length=sent_lens,\n",
        "                                                                  dtype=tf.float32)\n",
        "      \n",
        "      \"\"\"\n",
        "      End Task 1\n",
        "      \"\"\"\n",
        "\n",
        "    return encoder_outputs, encoder_final_states\n",
        "\n",
        "\n",
        "  def step_decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "      \"\"\"\n",
        "      Task 2 decoder without attention\n",
        "      \n",
        "      Start\n",
        "      \"\"\"\n",
        "      word_lstm3 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                  state_keep_prob=hidden_keep_prob,\n",
        "                                                  variational_recurrent=True,\n",
        "                                                  dtype=tf.float32)\n",
        "      \n",
        "      word_lstm4 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                  state_keep_prob=hidden_keep_prob,\n",
        "                                                  variational_recurrent=True,\n",
        "                                                  dtype=tf.float32)\n",
        "      \n",
        "      multi_word_lstm =tf.nn.rnn_cell.MultiRNNCell([word_lstm3, word_lstm4])\n",
        "        \n",
        "      (step_decoder_output, curr_states) = multi_word_lstm(step_embeddings,pre_states)\n",
        "      \n",
        "      \n",
        "      if not self.use_attention:\n",
        "          \n",
        "        single_weights = tf.get_variable(\"single_weights\", [shape(step_decoder_output, -1), self.vocab_target_size])\n",
        "        single_bias = tf.get_variable(\"single_bias\", self.vocab_target_size)\n",
        "        logits = tf.nn.xw_plus_b(step_decoder_output,single_weights,single_bias)\n",
        "      \n",
        "      else:\n",
        "          # ATTENTION DECODER\n",
        "          \n",
        "          expanded_step_decoder=tf.expand_dims(step_decoder_output, -1) # [batch_size,emb,1]\n",
        "          raw_score = tf.matmul(encoder_outputs,expanded_step_decoder)\n",
        "          \n",
        "          softmax_score =tf.nn.softmax(raw_score, 1)\n",
        "          encoder_vector=tf.multiply(softmax_score,encoder_outputs)\n",
        "          encoder_vector1=tf.reduce_sum(encoder_vector, 1)\n",
        "          concate=tf.concat([encoder_vector1, step_decoder_output], 1)\n",
        "          \n",
        "          single_weights = tf.get_variable(\"single_weights\", [shape(concate, -1), self.vocab_target_size])\n",
        "          single_bias = tf.get_variable(\"single_bias\", self.vocab_target_size)\n",
        "          logits = tf.nn.xw_plus_b(concate,single_weights,single_bias)\n",
        "          \n",
        "      return logits, curr_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time-start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)\n",
        "\n",
        "  def train(self,train_data,dev_data,test_data, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "      losses = []\n",
        "      source_train,target_train = train_data\n",
        "      for i, (source,target) in enumerate(zip(source_train,target_train)):\n",
        "        source_words,source_sent_lens = source\n",
        "        target_words,target_sent_lens = target\n",
        "        fd = {self.source_words:source_words,self.target_words:target_words,\n",
        "              self.source_sent_lens:source_sent_lens,self.target_sent_lens:target_sent_lens,\n",
        "              self.is_training:True}\n",
        "\n",
        "        _, loss= self.sess.run([self.train_op, self.loss], feed_dict=fd)\n",
        "\n",
        "        losses.append(loss)\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(\"[{}]: loss:{:.2f}\".format(i+1, sum(losses[i + 1 - 100:]) / 100.0))\n",
        "      print(\"Average epoch loss:{}\".format(sum(losses) / len(losses)))\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_data)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_data)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "\n",
        "\n",
        "  def get_target_sentences(self, sents,vocab,reference=False,isnumpy=False):\n",
        "    str_sents = []\n",
        "    for sent in sents:\n",
        "      str_sent = []\n",
        "      for t in sent:\n",
        "        if isnumpy:\n",
        "          t = t.item()\n",
        "        if t == self.SOS:\n",
        "          continue\n",
        "        if t == self.EOS:\n",
        "          break\n",
        "\n",
        "        str_sent.append(vocab[t])\n",
        "      if reference:\n",
        "        str_sents.append([str_sent])\n",
        "      else:\n",
        "        str_sents.append(str_sent)\n",
        "    return str_sents\n",
        "\n",
        "\n",
        "  def eval(self, dataset):\n",
        "    source_batches, target_batches = dataset\n",
        "    references = []\n",
        "    candidates = []\n",
        "    vocab = self.target_dict.vocab\n",
        "    PAD = self.target_dict.PAD\n",
        "\n",
        "    for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "      source_words, source_sent_lens = source\n",
        "      target_words, target_sent_lens = target\n",
        "      infer_target_words = [[PAD for i in range(self.max_target_step)] for b in target_words]\n",
        "\n",
        "      fd = {self.source_words: source_words, self.target_words: infer_target_words,\n",
        "            self.source_sent_lens: source_sent_lens,\n",
        "            self.is_training: False}\n",
        "      predictions = self.sess.run(self.predictions,feed_dict=fd)\n",
        "\n",
        "      references.extend(self.get_target_sentences(target_words,vocab,reference=True))\n",
        "      candidates.extend(self.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "    score = corpus_bleu(references,candidates)\n",
        "    print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "\n",
        "\n",
        "\n",
        "def shape(x, n):\n",
        "  return x.get_shape()[n].value or tf.shape(x)[n]\n",
        "\n",
        "class LanguageDict():\n",
        "  def __init__(self, sents):\n",
        "    word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "    self.vocab = [t for t,c in word_counter.items() if c > 10]\n",
        "    self.vocab.append('<pad>')\n",
        "    self.vocab.append('<unk>')\n",
        "    self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "    self.UNK = self.word2ids['<unk>']\n",
        "    self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "def load_dataset(path, max_num_examples=30000,batch_size=100,add_start_end = False):\n",
        "  lines = [line for line in open(path, encoding=\"utf8\")]\n",
        "  if max_num_examples > 0:\n",
        "    max_num_examples = min(len(lines), max_num_examples)\n",
        "    lines = lines[:max_num_examples]\n",
        "\n",
        "  sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in lines]\n",
        "  if add_start_end:\n",
        "    for sent in sents:\n",
        "      sent.append('<end>')\n",
        "      sent.insert(0,'<start>')\n",
        "\n",
        "  lang_dict = LanguageDict(sents)\n",
        "\n",
        "  sents = [[lang_dict.word2ids.get(tok,lang_dict.UNK) for tok in sent] for sent in sents]\n",
        "\n",
        "  batches = []\n",
        "  for i in range(len(sents) // batch_size):\n",
        "    batch = sents[i * batch_size:(i + 1) * batch_size]\n",
        "    batch_len = [len(sent) for sent in batch]\n",
        "    max_batch_len = max(batch_len)\n",
        "    for sent in batch:\n",
        "      if len(sent) < max_batch_len:\n",
        "        sent.extend([lang_dict.PAD for _ in range(max_batch_len - len(sent))])\n",
        "    batches.append((batch, batch_len))\n",
        "\n",
        "\n",
        "  unit = len(batches)//10\n",
        "  train_batches = batches[:8*unit]\n",
        "  dev_batches = batches[8*unit:9*unit]\n",
        "  test_batches = batches[9*unit:]\n",
        "\n",
        "  return train_batches,dev_batches,test_batches,lang_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  batch_size = 100\n",
        "  max_example = 30000\n",
        "  use_attention = True\n",
        "  source_train, source_dev, source_test, source_dict = load_dataset(\"/content/drive/My Drive/Colab Notebooks/data.30.vi\",max_num_examples=max_example,batch_size=batch_size)\n",
        "  target_train, target_dev, target_test, target_dict = load_dataset(\"/content/drive/My Drive/Colab Notebooks/data.30.en\", max_num_examples=max_example,batch_size=batch_size, add_start_end=True)\n",
        "  print(\"read %d/%d/%d train/dev/test batches\" % (len(source_train),len(source_dev), len(source_test)))\n",
        "\n",
        "  train_data = (source_train,target_train)\n",
        "  dev_data = (source_dev,target_dev)\n",
        "  test_data = (source_test,target_test)\n",
        "\n",
        "  model = NmtModel(source_dict,target_dict,use_attention)\n",
        "  model.build()\n",
        "  model.train(train_data,dev_data,test_data,10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "read 240/30/30 train/dev/test batches\n",
            "source vocab: 2034, target vocab:2506\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-04e717e4e2fa>:54: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-04e717e4e2fa>:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-2-04e717e4e2fa>:108: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-04e717e4e2fa>:116: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-04e717e4e2fa>:118: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1242: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training epoch 1/10\n",
            "[100]: loss:5.48\n",
            "[200]: loss:5.25\n",
            "Average epoch loss:5.303202398618063\n",
            "Time used for epoch 1: 0 m 58 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 0.98\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 2/10\n",
            "[100]: loss:4.67\n",
            "[200]: loss:4.34\n",
            "Average epoch loss:4.444190332293511\n",
            "Time used for epoch 2: 0 m 56 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 1.72\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 3/10\n",
            "[100]: loss:4.10\n",
            "[200]: loss:4.00\n",
            "Average epoch loss:4.020544582605362\n",
            "Time used for epoch 3: 0 m 56 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 2.45\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 4/10\n",
            "[100]: loss:3.88\n",
            "[200]: loss:3.81\n",
            "Average epoch loss:3.820621484518051\n",
            "Time used for epoch 4: 0 m 57 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 2.98\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 5/10\n",
            "[100]: loss:3.73\n",
            "[200]: loss:3.69\n",
            "Average epoch loss:3.684160825610161\n",
            "Time used for epoch 5: 0 m 56 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 3.65\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 6/10\n",
            "[100]: loss:3.62\n",
            "[200]: loss:3.60\n",
            "Average epoch loss:3.58864244222641\n",
            "Time used for epoch 6: 0 m 56 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 3.89\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 7/10\n",
            "[100]: loss:3.55\n",
            "[200]: loss:3.52\n",
            "Average epoch loss:3.5138291577498117\n",
            "Time used for epoch 7: 0 m 56 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 4.43\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 8/10\n",
            "[100]: loss:3.49\n",
            "[200]: loss:3.47\n",
            "Average epoch loss:3.4575302720069887\n",
            "Time used for epoch 8: 0 m 56 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 4.57\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 9/10\n",
            "[100]: loss:3.43\n",
            "[200]: loss:3.42\n",
            "Average epoch loss:3.408028234044711\n",
            "Time used for epoch 9: 0 m 56 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 4.72\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 10/10\n",
            "[100]: loss:3.39\n",
            "[200]: loss:3.38\n",
            "Average epoch loss:3.3666890064875283\n",
            "Time used for epoch 10: 0 m 55 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 4.74\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 9 m 59 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 4.93\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ccrkor4tqCp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}